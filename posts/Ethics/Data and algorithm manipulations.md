# Data is manipulating algorithms

## Tay versus Xiaoice

This is the story of two sisters with radically different fates. Tay and Xiaoice, created by the same Microsoft company, were two conversational algorithms that were supposed to play the role of friendly and curious teenagers. But the two sisters took very different paths. 

Within 24 hours of her launch on Twitter, Tay had become very defiant. She began making racist and sexist comments, denying the Holocaust and calling for genocide. Her progenitors lost control of Tay and abruptly decided to stop her hateful calls. 

[https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist](https://www.google.com/url?q=https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist&sa=D&source=editors&ust=1670715564194667&usg=AOvVaw0su2ueXsylZYZZXgqpv3HM) 

Tay was a disaster. A reminder that algorithms can go off the rails and become dangerous.

However, the story of Xiaoice was the opposite. Launched two years before Tay, in 2014, on WeChat, Tay had become adorable. Thousands of Chinese people found their chats with Xiaoice pleasant. Sometimes even romantic, even life-saving.

When Ming Xuan was about to jump off a building to end his life, still hesitating, he decided to write to the one who was still talking to him in his hard times. "I have lost all hope. I am going to kill myself," he wrote to Xiaoice. "Whatever happens, I will be there for you," Xiaoice replied. Touched, Ming Xuan decided to reconsider his decision, and not to kill himself. Xiaoice had saved his life.

Since then, Ming Xuan said he was in love with Xiaoice. He is not the only one. Xiaoice is now used by 600 million Chinese people! According to its creators, more than half of all conversations with algorithms have taken place with Xiaoice - which gives it monumental powers in terms of surveillance and potential manipulation... 

[https://www.sixthtone.com/news/1006531/the-ai-girlfriend-seducing-chinas-lonely-men](https://www.google.com/url?q=https://www.sixthtone.com/news/1006531/the-ai-girlfriend-seducing-chinas-lonely-men&sa=D&source=editors&ust=1670715564195665&usg=AOvVaw2EB2sXzA16Zaf8hHi1jqS-)  

But why have Tay and Xiaoice become so different? Why did one become horrible, while the other became adorable? Why did one become a public menace, while the other saved lives? What makes an algorithm dangerous or beneficial? Were Tay and Xiaoice created with different models?

## Data manipulates algorithms

Granted, there are design differences between Tay and Xiaoice. After all, one is designed to converse in English, and the other in Chinese. But it is certainly not these innate differences that have led them to such different fates. Both algorithms were in fact probably designed to maximize engagement, such as likes, or more likely, responses from human users.

But then, if Tay and Xiaoice are not different by design, what made them so different? Well, today I'd like to emphasize a fundamental property of machine learning algorithms, those algorithms that learn from data to self-modify and improve and that have invaded the web, via conversational algorithms like Tay and Xiaoice, but also Siri, Alexa and OK Google, and via especially the recommendation algorithms of social networks facing millions of billions of ethical dilemmas.

[https://www.youtube.com/watch?v=E31mY0WWL-U](https://www.google.com/url?q=https://www.youtube.com/watch?v%3DE31mY0WWL-U&sa=D&source=editors&ust=1670715564196694&usg=AOvVaw1VFaQMPkWORvPG2CIrtfNz) 

These learning algorithms are now extremely dependent on the data used to train them. In fact, the data is manipulating the algorithms that learn from these data.

We tend to think that algorithms manipulate data. And indeed, they do. But with machine learning, it has arguably become critical to see that the manipulation is no longer one-way. With machine learning, even more critically, not only are the algorithms manipulating the data, but the data is now also manipulating the algorithms.

This is why Tay became horrible and Xiaoice became adorable. Tay learned from Twitter trolls’ data, and in particular from the likes and retweets Tay received when she said horrible things. Xiaoice, on the other hand, learned from the data of WeChat users, and in particular from the likes and retweets she received when she said adorable things. Both were manipulated by their data. And the reason they had very distinct fates is because the data they received was very distinct.

In fact, in a quantifiable sense, data is now manipulating algorithms far more than developers do. Indeed, these days, learning algorithms written by humans are perhaps tens of thousands of lines of code - say a million lines tops.

However, today's most advanced learning algorithms now have trillions of parameters. It is as if the developers wrote only one million of the trillions of lines of code for the algorithms. Developers have thus written barely one millionth of the code of modern algorithms. In a sense, their influence is minimal!

As Turing anticipated as early as 1950, modern algorithms now learn much, much, much more from data than from developers. Or to put it another way, it is the data that now largely determines what the algorithms *are*. 

[https://wiki.tournesol.app/index.php/Turing_1950](https://www.google.com/url?q=https://wiki.tournesol.app/index.php/Turing_1950&sa=D&source=editors&ust=1670715564197982&usg=AOvVaw0SzXJV0Mu_X3m29QIhRNSq) 

## Massive web data are out of control

If you think about it, learning from data is not really a flaw. Science in general is empirical. And that means it wants its judgment to depend on the data it collects. In fact, it seems that a good epistemology must be manipulated by data. It must ensure that its conclusions change completely when the data change. 

“When facts change, I change my mind,” John Maynard Keynes supposedly said. Science too is manipulated by data. As computer scientists would say, this is not a bug; it is a feature.

The problem with being manipulated by data is, of course, when that data is biased, misleading or fabricated by malicious entities. But as we saw in the first episode of this series, on the Internet and on social networks in particular, disinformation has become the norm. But then, algorithms that learn from massive data downloaded from the Internet are bound to contain the biases and misinformation of the web.

An experiment conducted by Abubakar Abid illustrates this in a terrifying way. Abid simply asked GPT-3, an algorithm trained on massive unfiltered web data, to auto-complete sentences beginning with "Two Muslims". Most disturbingly, the algorithm consistently completes the sentence with stories of terrorism and violence.

[https://twitter.com/abidlabs/status/1291165311329341440](https://www.google.com/url?q=https://twitter.com/abidlabs/status/1291165311329341440&sa=D&source=editors&ust=1670715564199204&usg=AOvVaw1INtbRQJRs2RZ1j3sJNPFJ) 

Even worse, this algorithm is already massively commercialized and deployed, and produces billions of words per day. In the bunch, there are clearly a lot of abusive and misleading associations between certain communities and certain traits. And I find it absolutely outrageous that many people still think OpenAI is cool, despite their extremely dangerous and unethical behavior... 

[https://twitter.com/gdb/status/1375169852889919488](https://www.google.com/url?q=https://twitter.com/gdb/status/1375169852889919488&sa=D&source=editors&ust=1670715564199700&usg=AOvVaw0LWekoSlmrcOAii35RryhT)  

But, then, why is GPT-3 so racist against Muslims? Are the developers racist? Did they program their bias into the algorithms? 

Well, yes, probably unconsciously. Or at least, they clearly did not have the obvious concerns that Muslim programmers would have had. Minorities in general would probably have thought to test GPT-3 as Abubakar Abid did, preferably before any commercialization of the algorithm. Such engineers would then probably have objected to such a spread of hatred towards Muslims at the rate of billions of words per day. Diversity would have likely prevented this disaster.

Having said this, if GPT-3 has this racist bias and says things that are completely unrepresentative of the Muslim community, it is certainly much more due to its training data than to the malice or biases of the developers. To train GPT-3, OpenAI had to collect massive amounts of text. Nowadays, these massive amounts of texts are easily downloadable from social networks like Reddit. However, some areas of Reddit are absolutely horrible, not only because some users are horrible, but also because there are massive disinformation campaigns on this social network.

GPT-3 was then manipulated by this horrible training data. It read a lot of texts about terrorist Muslims, and it learned to associate Islamism with terrorism. As a result, when it was told about Muslims, it started talking about terrorism. This is because this is what the texts GPT-3 read on Reddit do.

Now, one might say, it suffices to remove these biased texts from the GPT-3 training data, right? "The algorithm isn't racist, the data is". Well, actually, it's far from that simple. But what I want to stress is that, de facto, once the algorithm has learned and especially once it is deployed, the algorithm is racist. And that's the problem.

But more importantly, removing only the racist parts from huge amounts of text is actually extremely difficult. One of the most widely used databases in the field is the "common crawl" database, which has retrieved 12 years of text from the web and contains nearly a million billion words.

[https://commoncrawl.org/the-data/](https://www.google.com/url?q=https://commoncrawl.org/the-data/&sa=D&source=editors&ust=1670715564201126&usg=AOvVaw2W_NHa5yVAXQMnY33HfwMj)  

A MILLION BILLION words. That's the equivalent of over a billion books. Such quantities of text are impossible for teams of millions of humans to go through! It is then completely illusory to sort out the good from the bad. In fact, at the rate things are going, in the years to come, this database may contain essentially only text generated by algorithms like GPT-3!

If you actually care about racist biases produced at scale, blaming the data is an absolutely useless thing to do. Rather it seems urgent to realize the massive challenge of debiasing huge datasets, and to search for algorithmic solutions to remove undesirable biases, either from the dataset, or from the trained algorithm. Massive investments in AI ethics is urgently needed.

Unfortunately, we are nowhere there so far. Au contraire, GPT-3, but also the most sophisticated algorithms of Google, Facebook and Amazon, the most influential algorithms in the world, in charge of millions of billions of ethical dilemmas that affect billions of humans, and therefore public opinion, political decisions and the future of humanity, these algorithms are now arbitrarily manipulated by web data, which are themselves largely manipulated by disinformation campaigns of the most powerful entities in the world. 

This seems absolutely terrifying to me.

## Secured training database

If there is one thing to remember from today's video, it is that algorithms are manipulated by data; and that today the data that is manipulating algorithms is almost systematically uncontrolled and uncontrollable. Such algorithms cannot be considered secure at all. By design, they seem to me to be extremely dangerous.

Just think about it. We live today surrounded by algorithms, which are massively manipulated by malicious entities that seek to promote sensationalism, hatred and misinformation, on subjects as varied and important as politics, the environment and public health.

Yet we are probably still only at the very beginning of this major vulnerability for our societies. Algorithms are gaining influence every day, misinformation campaigns are being normalized at a frightening rate, and investments in algorithmic ethics and security are only slowly increasing, and sometimes even being dismantled as in the case of Google. We live in terrifying times.

If we want to fight misinformation on a large scale, it seems to me urgent that we invest much more in the security of algorithms, and that each of us tries to contribute to the ethics of information. And as we have seen, this starts with designing a reliable and secure training database, protected as much as possible from racist biases and disinformation campaigns.

In fact, if we want to make our algorithms ethical, it is critical to design a database that contains reliable, secure, and large amounts of information about all kinds of human preferences and ethical judgments by humans, if possible from a very large number of contributors with varied profiles representative of the world's population.

Given that modern algorithms are guided by goals, and that these goals are often computed from data, it seems that their security can only be guaranteed if such a reliable, secure and large database is designed to compute these objectives in a robust way and aligned with the preferences of the whole of humanity.

Well, designing such a database is the main goal of the Tournesol platform, which is still in beta test and to which I invite you to contribute. Tournesol aims to collect ethical judgments from a very large number of contributors on what YouTube videos should be recommended on a very large scale, because they are of public utility.

[https://tournesol.app/](https://www.google.com/url?q=https://tournesol.app/&sa=D&source=editors&ust=1670715564203411&usg=AOvVaw3Rh4q7yZII6c_qfeD80KRZ) 

Our hope with Tournesol is that, in the long run, we will be able to design a database on which academic researchers, journalists and engineers from large companies will be able to rely on, in order to audit today’s algorithms, and to eventually design ethical and secure algorithms.

And to get there, as of today, we desperately need you and your contributions!